{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba4efbb0",
   "metadata": {},
   "source": [
    "# baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bb8cc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel 'Python 3 (ipykernel)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. Unable to get resolved server information for google.colab:colab:d10cbae9-f2d4-448d-b438-b73eba258c2c"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "# å¿½ç•¥ pandas çš„ä¸€äº› FutureWarning\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# æ˜¾ç¤ºé…ç½®\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"âœ… åº“å¯¼å…¥å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf72063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_delhivery_trip_legs(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    å°†åŸå§‹çš„ Segment (ç‰‡æ®µ) çº§æ•°æ®èšåˆæˆ Trip-Leg (è·¯æ®µ) çº§æ•°æ®ã€‚\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # æ—¶é—´è½¬æ¢\n",
    "    for c in ['trip_creation_time', 'od_start_time', 'od_end_time']:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_datetime(df[c], errors='coerce')\n",
    "            \n",
    "    # åˆæ­¥è¿‡æ»¤æ— æ•ˆæ•°æ®\n",
    "    if set(['osrm_time','start_scan_to_end_scan']).issubset(df.columns):\n",
    "        df = df[(df['osrm_time'] > 0) & (df['start_scan_to_end_scan'] >= 0)]\n",
    "        \n",
    "    # å®šä¹‰èšåˆé”®\n",
    "    grp_keys = [k for k in ['trip_uuid','source_center','destination_center'] if k in df.columns]\n",
    "    \n",
    "    # èšåˆé€»è¾‘\n",
    "    agg = {\n",
    "        'actual_time': 'max',          # ç´¯ç§¯å€¼å–æœ€å¤§\n",
    "        'osrm_time': 'max',            # ç´¯ç§¯å€¼å–æœ€å¤§\n",
    "        'start_scan_to_end_scan': 'first', # æ ¡éªŒå€¼\n",
    "        'actual_distance_to_destination': 'last', # åº”æ¥è¿‘0\n",
    "        'osrm_distance': 'max',\n",
    "        'segment_actual_time': 'sum',  # ç”¨äºæ ¡éªŒ\n",
    "        'segment_osrm_time': 'sum',\n",
    "        'trip_creation_time': 'first',\n",
    "        'od_start_time': 'first',      # å‘è½¦æ—¶é—´\n",
    "        'route_type': 'first',\n",
    "        'source_name': 'first',\n",
    "        'destination_name': 'first'\n",
    "    }\n",
    "    \n",
    "    cols = [c for c in agg.keys() if c in df.columns]\n",
    "    agg_use = {c: agg[c] for c in cols}\n",
    "    \n",
    "    # æ‰§è¡Œèšåˆ\n",
    "    g = df.groupby(grp_keys, dropna=False)\n",
    "    out = g.agg(agg_use)\n",
    "    out['num_segments'] = g.size().values # è®¡ç®—ç‰‡æ®µæ•°é‡\n",
    "    out = out.reset_index()\n",
    "    \n",
    "    return out\n",
    "\n",
    "print(\"âœ… èšåˆå‡½æ•°å®šä¹‰å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa6a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®æº URL (ä½¿ç”¨æ‚¨æä¾›çš„ GitHub Raw é“¾æ¥)\n",
    "DATA_FILENAME = 'delhivery_data.csv'\n",
    "DATA_URL = 'https://raw.githubusercontent.com/Tsingbbby/fx/master/å¤§ä½œä¸š/delhivery_data.csv'\n",
    "\n",
    "# 1. ä¸‹è½½æ•°æ®\n",
    "if not os.path.exists(DATA_FILENAME):\n",
    "    print(f\"ğŸ“¥ æ­£åœ¨ä¸‹è½½æ•°æ®: {DATA_URL} ...\")\n",
    "    !wget -q -O {DATA_FILENAME} {DATA_URL}\n",
    "    print(\"âœ… ä¸‹è½½å®Œæˆ\")\n",
    "else:\n",
    "    print(f\"âœ… æ–‡ä»¶å·²å­˜åœ¨: {DATA_FILENAME}\")\n",
    "\n",
    "# 2. è¯»å–ä¸è¿‡æ»¤\n",
    "print(\"ğŸ”„ æ­£åœ¨è¯»å–æ•°æ®...\")\n",
    "raw = pd.read_csv(DATA_FILENAME)\n",
    "\n",
    "# åªä¿ç•™ training æ•°æ®ï¼ˆå¦‚æœæœ‰ data åˆ—åŒºåˆ†ï¼‰\n",
    "df_raw = raw[raw['data'] == 'training'] if 'data' in raw.columns else raw\n",
    "\n",
    "# 3. æ‰§è¡Œèšåˆ\n",
    "print(\"ğŸ”„ æ­£åœ¨èšåˆ Segment -> Trip Leg ...\")\n",
    "legs = aggregate_delhivery_trip_legs(df_raw)\n",
    "\n",
    "print(f\"âœ… èšåˆå®Œæˆã€‚åŸå§‹è·¯æ®µæ•°é‡: {legs.shape[0]}\")\n",
    "legs.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7053eda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_features(df):\n",
    "    x = df.copy()\n",
    "    \n",
    "    # 1. åŸºç¡€æœ‰æ•ˆæ€§è¿‡æ»¤\n",
    "    # OSRM æ—¶é—´å¿…é¡»å¤§äº 0\n",
    "    x = x[x['osrm_time'] > 0]\n",
    "    \n",
    "    # 2. å¾®å‹è¡Œç¨‹è¿‡æ»¤ (Micro-trips)\n",
    "    # å‰”é™¤é¢„ä¼°æ—¶é—´ < 10åˆ†é’Ÿ æˆ– è·ç¦» < 5km çš„å•å­ï¼Œè¿™äº›é€šå¸¸æ˜¯æŒªè½¦æˆ–ç³»ç»Ÿè¯¯å·®\n",
    "    x = x[(x['osrm_time'] >= 10) & (x['osrm_distance'] >= 5)]\n",
    "    \n",
    "    # 3. ç‰©ç†æé™è¿‡æ»¤\n",
    "    # è®¡ç®—å¹³å‡é€Ÿåº¦ (km/h)\n",
    "    x['speed_kmh'] = x['osrm_distance'] / (x['actual_time'] / 60.0)\n",
    "    \n",
    "    # å‰”é™¤é£è½¦ (> 120 km/h)\n",
    "    x = x[x['speed_kmh'] <= 120]\n",
    "    \n",
    "    # å‰”é™¤é•¿æœŸæ»ç•™/æœªå…³é—­è®¢å• (é€Ÿåº¦ < 1 km/h ä¸” æ—¶é•¿ > 10å°æ—¶)\n",
    "    # è¿™ç±»é€šå¸¸æ˜¯è®¾å¤‡å¿˜äº†å…³ï¼Œå±äºè„æ•°æ®\n",
    "    x = x[~((x['speed_kmh'] < 1) & (x['actual_time'] >= 600))]\n",
    "    \n",
    "    return x\n",
    "\n",
    "legs_clean = sanitize_features(legs)\n",
    "print(f\"ğŸ§¹ æ¸…æ´—åæœ‰æ•ˆæ ·æœ¬æ•°: {legs_clean.shape[0]} (å‰”é™¤äº† {legs.shape[0] - legs_clean.shape[0]} æ¡è„æ•°æ®)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1890c27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_target_and_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. æ ‡ç­¾æ„å»º\n",
    "    # å®šä¹‰å»¶è¯¯æ¯”ç‡\n",
    "    df['delay_ratio'] = df['actual_time'] / df['osrm_time']\n",
    "    \n",
    "    # å®šä¹‰ä¸¥é‡å»¶è¯¯ (Target = 1 if Ratio > 2.0)\n",
    "    # è¿™æ˜¯ä¸€ä¸ªå¤©ç„¶å¹³è¡¡çš„é˜ˆå€¼ (çº¦50%æ­£æ ·æœ¬)\n",
    "    df['is_severe_delay'] = (df['delay_ratio'] > 2.0).astype(int)\n",
    "    \n",
    "    # 2. åŸºç¡€æ—¶é—´ç‰¹å¾\n",
    "    ts_col = 'od_start_time'\n",
    "    df[ts_col] = pd.to_datetime(df[ts_col])\n",
    "    \n",
    "    df['hour'] = df[ts_col].dt.hour\n",
    "    df['dayofweek'] = df[ts_col].dt.dayofweek\n",
    "    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n",
    "    \n",
    "    # æŒ‰æ—¶é—´æ’åº (éå¸¸é‡è¦ï¼Œé˜²æ­¢ç©¿è¶Š)\n",
    "    df = df.sort_values(ts_col)\n",
    "    \n",
    "    return df\n",
    "\n",
    "final_df = build_target_and_features(legs_clean)\n",
    "print(\"âœ… æ ‡ç­¾ä¸åŸºç¡€ç‰¹å¾æ„å»ºå®Œæˆ\")\n",
    "print(f\"å…¨å±€ä¸¥é‡å»¶è¯¯ç‡: {final_df['is_severe_delay'].mean():.4f}\")\n",
    "final_df[['od_start_time', 'actual_time', 'osrm_time', 'delay_ratio', 'is_severe_delay']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e51b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ä¿®å¤ç‰ˆ Cell 6: åŠ¨æ€æ—¶åºåˆ‡åˆ† ---\n",
    "\n",
    "# 1. ç¡®ä¿æŒ‰æ—¶é—´æ’åº\n",
    "final_df = final_df.sort_values('od_start_time').reset_index(drop=True)\n",
    "\n",
    "# 2. è·å–æ—¶é—´åˆ—è¡¨\n",
    "times = final_df['od_start_time']\n",
    "total_len = len(final_df)\n",
    "\n",
    "# 3. è®¡ç®—åˆ‡åˆ†ç‚¹ç´¢å¼• (80% / 10% / 10%)\n",
    "train_idx = int(total_len * 0.80)\n",
    "val_idx = int(total_len * 0.90)\n",
    "\n",
    "# 4. è·å–åˆ‡åˆ†æ—¥æœŸ (ç”¨äºæ‰“å°æ˜¾ç¤º)\n",
    "split_date_1 = times.iloc[train_idx]\n",
    "split_date_2 = times.iloc[val_idx]\n",
    "\n",
    "# 5. æ‰§è¡Œåˆ‡åˆ†\n",
    "# æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨ iloc åŸºäºä½ç½®åˆ‡åˆ†ï¼Œç¡®ä¿æ¯”ä¾‹ä¸¥æ ¼æ­£ç¡®ï¼Œä¸å—æ—¥æœŸç¼ºå¤±å½±å“\n",
    "df_train = final_df.iloc[:train_idx].copy()\n",
    "df_val   = final_df.iloc[train_idx:val_idx].copy()\n",
    "df_test  = final_df.iloc[val_idx:].copy()\n",
    "\n",
    "# 6. æ‰“å°æŠ¥å‘Š\n",
    "print(f\"ğŸ“… åŠ¨æ€åˆ‡åˆ†æ—¶é—´ç‚¹:\")\n",
    "print(f\"   Train ç»“æŸäº: {split_date_1}\")\n",
    "print(f\"   Val   ç»“æŸäº: {split_date_2}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"ğŸ“Š æ•°æ®é›†åˆ‡åˆ†æƒ…å†µ (Target: ä¸¥é‡å»¶è¯¯ç‡):\")\n",
    "print(f\"   Train : {len(df_train):5d} æ ·æœ¬ | Target Mean: {df_train['is_severe_delay'].mean():.4f}\")\n",
    "print(f\"   Val   : {len(df_val):5d} æ ·æœ¬ | Target Mean: {df_val['is_severe_delay'].mean():.4f}\")\n",
    "print(f\"   Test  : {len(df_test):5d} æ ·æœ¬ | Target Mean: {df_test['is_severe_delay'].mean():.4f}\")\n",
    "\n",
    "# 7. éªŒè¯åˆ†å¸ƒç¨³å®šæ€§\n",
    "train_mean = df_train['is_severe_delay'].mean()\n",
    "test_mean = df_test['is_severe_delay'].mean()\n",
    "diff = abs(train_mean - test_mean)\n",
    "\n",
    "if diff > 0.15:\n",
    "    print(f\"\\nâš ï¸ è­¦å‘Š: è®­ç»ƒé›†ä¸æµ‹è¯•é›†åˆ†å¸ƒå·®å¼‚è¾ƒå¤§ (Diff: {diff:.2f})\")\n",
    "    print(\"è¿™å¯èƒ½æ˜¯ç”±äºè¿‘æœŸå‘ç”Ÿäº†ç‰¹æ®Šäº‹ä»¶ï¼ˆå¦‚å¤§ä¿ƒã€æ¶åŠ£å¤©æ°”ï¼‰ã€‚å»ºè®®åœ¨å»ºæ¨¡æ—¶åŠ å…¥ 'DayOfWeek' æˆ– 'Is_End_Month' ç‰¹å¾ã€‚\")\n",
    "else:\n",
    "    print(f\"\\nâœ… éªŒè¯é€šè¿‡: æ—¶åºåˆ†å¸ƒç›¸å¯¹ç¨³å®š (Diff: {diff:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99334fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y_true, y_pred, name):\n",
    "    return {\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'Recall': recall_score(y_true, y_pred),\n",
    "        'Precision': precision_score(y_true, y_pred),\n",
    "        'F1': f1_score(y_true, y_pred),\n",
    "        'AUC': roc_auc_score(y_true, y_pred) # å¯¹äºäºŒå€¼é¢„æµ‹ï¼ŒAUCç­‰äºå¹³è¡¡å‡†ç¡®ç‡\n",
    "    }\n",
    "\n",
    "metrics_list = []\n",
    "y_test = df_test['is_severe_delay']\n",
    "\n",
    "# Baseline 1: å…¨å‘˜æ­£å¸¸ (All Zeros) - æ¨¡æ‹Ÿä¸åšä»»ä½•é¢„æµ‹\n",
    "pred_naive1 = np.zeros_like(y_test)\n",
    "metrics_list.append(get_metrics(y_test, pred_naive1, \"Naive: All Normal\"))\n",
    "\n",
    "# Baseline 2: ç±»å‹åè§ (Cartingæ€»æ˜¯å»¶è¯¯, FTLæ€»æ˜¯æ­£å¸¸)\n",
    "pred_naive2 = np.where(df_test['route_type'] == 'Carting', 1, 0)\n",
    "metrics_list.append(get_metrics(y_test, pred_naive2, \"Naive: Route Type Rule\"))\n",
    "\n",
    "# Baseline 3: å†å²å»¶è¯¯ç‡ (Source Center History)\n",
    "# æ³¨æ„ï¼šåªèƒ½ç”¨ Train æ•°æ®è®¡ç®—å†å²æ¦‚ç‡ï¼Œé˜²æ­¢ç©¿è¶Š\n",
    "train_center_stats = df_train.groupby('source_center')['is_severe_delay'].mean()\n",
    "global_mean = df_train['is_severe_delay'].mean()\n",
    "\n",
    "# æ˜ å°„åˆ° Test é›†\n",
    "test_center_prob = df_test['source_center'].map(train_center_stats).fillna(global_mean)\n",
    "pred_naive3 = (test_center_prob > 0.5).astype(int)\n",
    "metrics_list.append(get_metrics(y_test, pred_naive3, \"Naive: Source History Rule\"))\n",
    "\n",
    "# å±•ç¤ºç»“æœ\n",
    "baseline_df = pd.DataFrame(metrics_list).set_index('Model')\n",
    "print(\"ğŸ“‰ å¯å‘å¼åŸºçº¿æ€§èƒ½ (Test Set):\")\n",
    "display(baseline_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3b8ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: ç‰¹å¾åŸå‹ - æ¢çº½æ‹¥å µæŒ‡æ•° (Fix Rolling Bug & Optimize)\n",
    "\n",
    "def calculate_hub_congestion(full_df, train_df):\n",
    "    # 1. å‡†å¤‡æ•°æ®ï¼šåªä¿ç•™éœ€è¦çš„åˆ—\n",
    "    # å¿…é¡»ç¡®ä¿ od_start_time æ˜¯ datetime ç±»å‹\n",
    "    temp = full_df[['source_center', 'od_start_time', 'is_severe_delay']].copy()\n",
    "    temp['od_start_time'] = pd.to_datetime(temp['od_start_time'])\n",
    "    temp = temp.sort_values(['source_center', 'od_start_time'])\n",
    "    \n",
    "    # 2. èšåˆåˆ° (Center, Time) çº§åˆ«\n",
    "    # å¤„ç†åŒä¸€ç§’å¤šè½¦å‡ºå‘çš„æƒ…å†µï¼Œå–å‡å€¼ï¼Œç¡®ä¿ç´¢å¼•å”¯ä¸€æ€§\n",
    "    grouped = temp.groupby(['source_center', 'od_start_time'])['is_severe_delay'].mean().reset_index()\n",
    "    \n",
    "    # 3. è®¾ç½®æ—¶é—´ç´¢å¼•ï¼Œè¿™æ˜¯ä½¿ç”¨ rolling('3D') çš„å‰æ\n",
    "    grouped = grouped.set_index('od_start_time').sort_index()\n",
    "    \n",
    "    print(\"â³ æ­£åœ¨è®¡ç®—æ»‘åŠ¨çª—å£ç‰¹å¾ (3D & 7D)...\")\n",
    "    \n",
    "    # 4. æ‰§è¡Œæ»šåŠ¨è®¡ç®— (Pandas ä¼˜åŒ–å†™æ³•)\n",
    "    # closed='left' æ ¸å¿ƒå‚æ•°ï¼šç¡®ä¿è®¡ç®—æ—¶ä¸åŒ…å«å½“å‰è¡Œï¼ˆé˜²æ­¢ç©¿è¶Šï¼Œåªçœ‹è¿‡å»ï¼‰\n",
    "    \n",
    "    # è®¡ç®— 3å¤©çª—å£\n",
    "    roll_3d = grouped.groupby('source_center')['is_severe_delay']\\\n",
    "                     .rolling(window='3D', min_periods=1, closed='left')\\\n",
    "                     .mean()\n",
    "    \n",
    "    # è®¡ç®— 7å¤©çª—å£\n",
    "    roll_7d = grouped.groupby('source_center')['is_severe_delay']\\\n",
    "                     .rolling(window='7D', min_periods=1, closed='left')\\\n",
    "                     .mean()\n",
    "    \n",
    "    # 5. å¤„ç†ç»“æœç´¢å¼•ä»¥ä¾¿åˆå¹¶\n",
    "    # roll_3d çš„ç´¢å¼•æ˜¯ MultiIndex (source_center, od_start_time)ï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶ reset ä¸ºåˆ—\n",
    "    roll_3d = roll_3d.rename('hub_congestion_3d').reset_index()\n",
    "    roll_7d = roll_7d.rename('hub_congestion_7d').reset_index()\n",
    "    \n",
    "    # 6. å°†è®¡ç®—ç»“æœåˆå¹¶å› grouped è¡¨\n",
    "    # grouped ç›®å‰ç´¢å¼•æ˜¯ od_start_timeï¼Œå…ˆ reset å›æ¥å˜æˆåˆ—\n",
    "    base_grouped = grouped.reset_index()\n",
    "    \n",
    "    # åˆå¹¶ç‰¹å¾\n",
    "    base_grouped = pd.merge(base_grouped, roll_3d, on=['source_center', 'od_start_time'], how='left')\n",
    "    base_grouped = pd.merge(base_grouped, roll_7d, on=['source_center', 'od_start_time'], how='left')\n",
    "    \n",
    "    # 7. å¡«å……å†·å¯åŠ¨ (Cold Start)\n",
    "    # ä½¿ç”¨è®­ç»ƒé›†çš„å…¨å±€å‡å€¼å¡«å…… NaN (é’ˆå¯¹é‚£äº›æ²¡æœ‰å†å²æ•°æ®çš„æ—©æœŸæ ·æœ¬)\n",
    "    global_mean = train_df['is_severe_delay'].mean()\n",
    "    base_grouped['hub_congestion_3d'] = base_grouped['hub_congestion_3d'].fillna(global_mean)\n",
    "    base_grouped['hub_congestion_7d'] = base_grouped['hub_congestion_7d'].fillna(global_mean)\n",
    "    \n",
    "    # 8. Merge å›åŸå§‹ full_df\n",
    "    # åŸå§‹æ•°æ®å¯èƒ½æ¯” grouped å¤šï¼ˆå› ä¸ºä¹‹å‰èšåˆè¿‡åŒä¸€ç§’çš„æ•°æ®ï¼‰ï¼Œç°åœ¨å¹¿æ’­å›å»\n",
    "    result_df = pd.merge(full_df, \n",
    "                         base_grouped[['source_center', 'od_start_time', 'hub_congestion_3d', 'hub_congestion_7d']], \n",
    "                         on=['source_center', 'od_start_time'], \n",
    "                         how='left')\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# æ‰§è¡Œç‰¹å¾è®¡ç®—\n",
    "# æ³¨æ„ï¼šè¿™ä¸€æ­¥å¯èƒ½éœ€è¦å‡ ç§’é’Ÿ\n",
    "final_df_feat = calculate_hub_congestion(final_df, df_train)\n",
    "\n",
    "print(\"âœ… æ¢çº½æ‹¥å µç‰¹å¾è®¡ç®—å®Œæˆ\")\n",
    "print(final_df_feat[['source_center', 'od_start_time', 'is_severe_delay', 'hub_congestion_3d']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbbb7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ” ä¸šåŠ¡æ´å¯Ÿåˆ†æ (EDA):\")\n",
    "\n",
    "# 1. è·¯çº¿æ‹“æ‰‘æŒ‡çº¹ (Route Topology)\n",
    "# æ£€æŸ¥åŒä¸€å¯¹ OD æ˜¯å¦æœ‰å¤šæ¡ä¸åŒçš„ OSRM è·ç¦»\n",
    "od_stats = final_df.groupby(['source_center', 'destination_center'])['osrm_distance'].nunique()\n",
    "multi_route_count = (od_stats > 1).sum()\n",
    "print(f\"\\n[è·¯çº¿å¤šæ ·æ€§]\")\n",
    "print(f\"æ€» OD å¯¹æ•°: {len(od_stats)}\")\n",
    "print(f\"æ‹¥æœ‰å¤šæ¡è·¯çº¿çš„ OD å¯¹æ•°: {multi_route_count} (å æ¯” {multi_route_count/len(od_stats):.2%})\")\n",
    "print(\"  -> è¿™æ„å‘³ç€ 'Route Choice' æœ¬èº«å¯èƒ½æ˜¯ä¸€ä¸ªéšå«ç‰¹å¾\")\n",
    "\n",
    "# 2. åç›´è§‰â€œç¥è½¦â€ (God Cars)\n",
    "# è·ç¦»å¾ˆè¿œ (>800km) ä½†å®é™…è·‘å¾—æ¯”ç®—æ³•è¿˜å¿« (Actual < OSRM)\n",
    "# ğŸ” ä¼˜åŒ–å®šä¹‰ï¼šæ¯”é¢„ä¼°æ—¶é—´å¿« 20% ä»¥ä¸Šï¼Œä¸”é•¿é€”\n",
    "god_cars = final_df[\n",
    "    (final_df['osrm_distance'] > 800) & \n",
    "    (final_df['delay_ratio'] < 0.8)  # æ–°å¢æ¡ä»¶\n",
    "]\n",
    "\n",
    "print(f\"\\n[åç›´è§‰æ¡ˆä¾‹]\")\n",
    "print(f\"é•¿é€”ä¸”å¿«äºç®—æ³•çš„å•é‡: {len(god_cars)}\")\n",
    "if len(god_cars) > 0:\n",
    "    print(\"Top 3 ç¥è½¦æ¡ˆä¾‹:\")\n",
    "    display(god_cars[['source_center', 'destination_center', 'osrm_distance', 'osrm_time', 'actual_time', 'route_type']].head(3))\n",
    "\n",
    "# 3. ç¦»è°±å»¶è¯¯ (Extreme Delays)\n",
    "top_delays = final_df.sort_values('delay_ratio', ascending=False).head(5)\n",
    "print(f\"\\n[Top 5 ä¸¥é‡å»¶è¯¯]\")\n",
    "display(top_delays[['source_center', 'destination_center', 'delay_ratio', 'actual_time', 'osrm_time', 'route_type']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412d8e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ğŸ› ï¸ ä¿®å¤æ­¥éª¤: åˆ·æ–°æ•°æ®é›† (åŒ…å«æ–°ç‰¹å¾) ---\n",
    "# åŸå› : ä¹‹å‰çš„ df_train æ˜¯åœ¨ç‰¹å¾è®¡ç®—å‰åˆ‡åˆ†çš„ï¼Œéœ€è¦ç”¨ final_df_feat é‡æ–°åˆ‡åˆ†\n",
    "\n",
    "if 'final_df_feat' not in locals():\n",
    "    raise ValueError(\"âŒ é”™è¯¯: æ‰¾ä¸åˆ° final_df_feat å˜é‡ã€‚è¯·åŠ¡å¿…å…ˆæˆåŠŸè¿è¡Œ Cell 8 (ç‰¹å¾è®¡ç®—)ï¼\")\n",
    "\n",
    "print(\"ğŸ”„ æ­£åœ¨åˆ·æ–° Train/Test æ•°æ®é›†ä»¥åŒ…å«æ–°ç‰¹å¾...\")\n",
    "\n",
    "# 1. ç¡®ä¿æŒ‰æ—¶é—´æ’åº\n",
    "final_df_feat = final_df_feat.sort_values('od_start_time').reset_index(drop=True)\n",
    "\n",
    "# 2. é‡æ–°è®¡ç®—åˆ‡åˆ†ç‚¹ (80% / 10% / 10%)\n",
    "total_len = len(final_df_feat)\n",
    "train_idx = int(total_len * 0.80)\n",
    "val_idx = int(total_len * 0.90)\n",
    "\n",
    "# 3. é‡æ–°èµ‹å€¼ df_train, df_test\n",
    "# è¿™æ ·å®ƒä»¬å°±æ‹¥æœ‰äº† hub_congestion_3d ç­‰æ–°åˆ—\n",
    "df_train = final_df_feat.iloc[:train_idx].copy()\n",
    "df_val   = final_df_feat.iloc[train_idx:val_idx].copy()\n",
    "df_test  = final_df_feat.iloc[val_idx:].copy()\n",
    "\n",
    "print(f\"âœ… æ•°æ®é›†åˆ·æ–°å®Œæˆã€‚df_train åˆ—æ•°: {df_train.shape[1]}\")\n",
    "\n",
    "# --- ä¿®å¤ä»£ç å¼€å§‹: ä½¿ç”¨æ­£ç¡®çš„é€»è¾‘æ£€æŸ¥ç‰¹å¾æ˜¯å¦å­˜åœ¨ ---\n",
    "check_feats = ['hub_congestion_3d', 'hub_congestion_7d']\n",
    "feats_exist = all(f in df_train.columns for f in check_feats)\n",
    "print(f\"   æ£€æŸ¥å…³é”®ç‰¹å¾æ˜¯å¦å­˜åœ¨: {feats_exist}\")\n",
    "# --- ä¿®å¤ä»£ç ç»“æŸ ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f70810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Phase 3: å»ºæ¨¡ä¸è¯„ä¼° (Modeling & Evaluation) ---\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "print(\"\\nğŸš€ è¿›å…¥ Phase 3: å»ºæ¨¡ä¸è¯„ä¼°...\")\n",
    "\n",
    "# 1. å‡†å¤‡ç‰¹å¾é›† (X) å’Œ æ ‡ç­¾ (y)\n",
    "NUM_COLS = [\n",
    "    'osrm_distance', 'osrm_time', 'num_segments', \n",
    "    'hub_congestion_3d', 'hub_congestion_7d', # åˆšæ‰æŠ¥é”™çš„ç‰¹å¾ç°åœ¨æœ‰äº†\n",
    "    'hour', 'dayofweek'\n",
    "]\n",
    "CAT_COLS = ['route_type', 'is_weekend']\n",
    "\n",
    "X_train = df_train[NUM_COLS + CAT_COLS]\n",
    "y_train = df_train['is_severe_delay']\n",
    "\n",
    "X_test = df_test[NUM_COLS + CAT_COLS]\n",
    "y_test = df_test['is_severe_delay']\n",
    "\n",
    "# 2. æ„å»ºé¢„å¤„ç†ç®¡é“\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), NUM_COLS),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), CAT_COLS)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 3. åˆå§‹åŒ– XGBoost æ¨¡å‹\n",
    "clf = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    scale_pos_weight=1, \n",
    "    eval_metric='auc',\n",
    "    use_label_encoder=False,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ç»„åˆ Pipeline\n",
    "model_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', clf)\n",
    "])\n",
    "\n",
    "# 4. è®­ç»ƒæ¨¡å‹\n",
    "print(f\"ğŸ”„ æ­£åœ¨è®­ç»ƒ XGBoost ({len(X_train)} æ ·æœ¬)...\")\n",
    "model_pipe.fit(X_train, y_train)\n",
    "print(\"âœ… è®­ç»ƒå®Œæˆ\")\n",
    "\n",
    "# 5. é¢„æµ‹ä¸è¯„ä¼°\n",
    "y_pred = model_pipe.predict(X_test)\n",
    "y_prob = model_pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nğŸ“Š --- æ¨¡å‹è¯„ä¼°æŠ¥å‘Š (Test Set) ---\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"AUC Score: {roc_auc_score(y_test, y_prob):.4f}\")\n",
    "\n",
    "# ç»˜åˆ¶æ··æ·†çŸ©é˜µ\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dceb6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Phase 4: å¯è§£é‡Šæ€§åˆ†æ (Explainability) ---\n",
    "print(\"\\nğŸ” è¿›å…¥ Phase 4: SHAP å¯è§£é‡Šæ€§åˆ†æ...\")\n",
    "\n",
    "# 1. æå–è½¬æ¢åçš„ç‰¹å¾å\n",
    "prep = model_pipe.named_steps['preprocessor']\n",
    "try:\n",
    "    cat_names = prep.named_transformers_['cat'].get_feature_names_out(CAT_COLS)\n",
    "except AttributeError:\n",
    "    cat_names = prep.named_transformers_['cat'].get_feature_names(CAT_COLS)\n",
    "    \n",
    "feature_names = NUM_COLS + list(cat_names)\n",
    "\n",
    "# 2. å‡†å¤‡ SHAP æ•°æ®\n",
    "X_test_transformed = prep.transform(X_test)\n",
    "\n",
    "# 3. è®¡ç®— SHAP å€¼\n",
    "explainer = shap.TreeExplainer(model_pipe.named_steps['classifier'])\n",
    "shap_values = explainer.shap_values(X_test_transformed)\n",
    "\n",
    "# 4. ç»˜åˆ¶ Summary Plot\n",
    "print(\"ğŸ“Š SHAP Summary Plot (å†³å®šå»¶è¯¯çš„å…³é”®å› ç´ ):\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(shap_values, X_test_transformed, feature_names=feature_names, show=False)\n",
    "plt.show()\n",
    "\n",
    "# 5. ä¸šåŠ¡æ´å¯Ÿç»“è®º\n",
    "importance_dict = dict(zip(feature_names, np.abs(shap_values).mean(0)))\n",
    "top_feature = max(importance_dict, key=importance_dict.get)\n",
    "print(\"\\nğŸ’¡ ä¸šåŠ¡æ´å¯Ÿ (Business Insight):\")\n",
    "print(f\"1. å¯¼è‡´ä¸¥é‡å»¶è¯¯çš„æœ€å…³é”®å› ç´ æ˜¯: [{top_feature}]\")\n",
    "print(\"2. è¯·è§‚å¯Ÿ Summary Plotï¼š\")\n",
    "print(\"   - å¦‚æœ Hub Congestion çš„çº¢ç‚¹(é«˜å€¼)åœ¨å³ä¾§(SHAP>0)ï¼Œè¯´æ˜è¯¥ç‰¹å¾ç¡®å®æ•æ‰åˆ°äº†æ‹¥å µä¼ å¯¼æ•ˆåº”ã€‚\")\n",
    "print(\"   - å¦‚æœ OSRM Distance çš„çº¢ç‚¹åœ¨å·¦ä¾§ï¼Œè¯´æ˜é•¿é€”å•åè€Œæ›´å‡†æ—¶ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb7ec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ğŸ“‰ 1: æˆ‘ä»¬åˆ°åº•é”™è¿‡äº†ä»€ä¹ˆï¼Ÿ(é”™è¯¯åˆ†æ) ---\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. æ‰¾å‡ºæ¼æŠ¥æ ·æœ¬ (False Negatives: å®é™…å»¶è¯¯ï¼Œä½†é¢„æµ‹æ­£å¸¸)\n",
    "mask_fn = (y_test == 1) & (y_pred == 0)\n",
    "fn_samples = X_test[mask_fn].copy()\n",
    "fn_samples['actual_ratio'] = df_test.loc[mask_fn, 'delay_ratio'] # å…³è”å›åŸå§‹ Ratio\n",
    "\n",
    "# 2. ç»˜åˆ¶æ¼æŠ¥æ ·æœ¬çš„å»¶è¯¯ç¨‹åº¦åˆ†å¸ƒ\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(fn_samples['actual_ratio'], bins=30, color='orange', kde=True)\n",
    "plt.axvline(2.0, color='red', linestyle='--', label='Threshold (2.0)')\n",
    "plt.title('Distribution of Delay Ratio for Missed Cases (False Negatives)')\n",
    "plt.xlabel('Actual Delay Ratio')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ’¡ æ•…äº‹è§£è¯»:\")\n",
    "print(\"å¦‚æœç›´æ–¹å›¾é›†ä¸­åœ¨ 2.0 - 2.5 ä¹‹é—´ï¼Œè¯´æ˜æ¨¡å‹åªæ˜¯åœ¨'è¾¹ç•Œ'ä¸ŠçŠ¹è±«ã€‚\")\n",
    "print(\"å¦‚æœæœ‰å¾ˆå¤š > 5.0 çš„æ ·æœ¬ä¹Ÿè¢«æ¼æŠ¥ï¼Œè¯´æ˜æ¨¡å‹å®Œå…¨æ²¡æœ‰æ•æ‰åˆ°æŸäº›æç«¯å¼‚å¸¸æ¨¡å¼ã€‚\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
